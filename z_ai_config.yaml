model_list:
  # Z.AI GLM Models - https://docs.litellm.ai/docs/providers/zai
  # API Endpoint: https://api.z.ai/api/coding/paas/v4/ (base URL for chat completions)
  - model_name: glm-4.6
    litellm_params:
      model: glm-4.6
      api_base: https://api.z.ai/api/coding/paas/v4/
      api_key: os.environ/ZAI_API_KEY
      custom_llm_provider: openai
      max_tokens: 8192  # Increased for better context handling
      # Disable reasoning for compatibility
      reasoning: false
      stream: true
      temperature: 0.7  # Balanced creativity/precision
      top_p: 0.9

  - model_name: glm-4.5
    litellm_params:
      model: glm-4.5
      api_base: https://api.z.ai/api/coding/paas/v4/
      api_key: os.environ/ZAI_API_KEY
      custom_llm_provider: openai
      max_tokens: 4096
      # Disable reasoning for compatibility
      reasoning: false
      stream: true

  - model_name: glm-4.5-flash
    litellm_params:
      model: glm-4.5-flash
      api_base: https://api.z.ai/api/coding/paas/v4/
      api_key: os.environ/ZAI_API_KEY
      custom_llm_provider: openai
      max_tokens: 4096
      # Disable reasoning for compatibility
      reasoning: false
      stream: true

  - model_name: glm-4.5-air
    litellm_params:
      model: glm-4.5-air
      api_base: https://api.z.ai/api/coding/paas/v4/
      api_key: os.environ/ZAI_API_KEY
      custom_llm_provider: openai
      max_tokens: 4096
      # Disable reasoning for compatibility
      reasoning: false
      stream: true

litellm_settings:
  # Pass through all parameters from Copilot to preserve instructions
  drop_params: false  # Changed to false to preserve Copilot's parameters
  success_callback: []
  failure_callback: []
  # Additional settings for better compatibility
  set_verbose: false
  json_logs: false
  # Timeout settings to prevent connection issues
  request_timeout: 600
  # Disable reasoning mode for compatibility with Copilot
  disable_reasoning: true
  # Rate limiting and retry settings
  num_retries: 3
  retry_after: 5  # seconds between retries
  # Rate limit per model to prevent 429 errors
  rpm: 20  # requests per minute per model
  # Enable fallback to other models on rate limit
  fallbacks: [{"glm-4.5-flash": ["glm-4.5-air", "glm-4.5", "glm-4.6"]}]
  # Preserve original request parameters
  allowed_fails: 0  # Strict mode - fail fast instead of modifying requests